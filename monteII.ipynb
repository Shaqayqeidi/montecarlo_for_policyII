{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "from collections import defaultdict\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset1():\n",
    "    st= [0]*2\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[0],weibull_shape[0])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun0(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[0])*tp[0]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[0])*time_interval * math.ceil(tf[0]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the argmax (key) and max (value) from a dictionary\n",
    "# put this into a function since we are using it so often\n",
    "def max_dict(d):  \n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALL_POSSIBLE_ACTIONS = [0,1]\n",
    "def policy_using_pi(St, pi):\n",
    "    return np.random.choice(ALL_POSSIBLE_ACTIONS, p=[pi[(St,a)] for a in ALL_POSSIBLE_ACTIONS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = list(range(0,100000+1,5))\n",
    "w = [0,1]\n",
    "states = list(itertools.product(age,w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_action(state , pi):\n",
    "    if (state[1]==1):\n",
    "        return  1  #repleace with probability 1\n",
    "    else: \n",
    "        return policy_using_pi(state, pi)  #epsilon_soft \n",
    "    \n",
    "\n",
    "choose_action((500,0) , pi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(1000):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step0(a_t,list(s_t))\n",
    "        r_t = rewardfun0(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step0(action , list(current_state))\n",
    "    reward = rewardfun0(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[1],weibull_shape[1])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun1(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[1])*tp[1]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[1])*time_interval * math.ceil(tf[1]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step1(a_t,list(s_t))\n",
    "        r_t = rewardfun1(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step1(action , list(current_state))\n",
    "    reward = rewardfun1(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Wheel Rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[2],weibull_shape[2])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun2(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[2])*tp[2]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[2])*time_interval * math.ceil(tf[2]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step2(a_t,list(s_t))\n",
    "        r_t = rewardfun2(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step2(action , list(current_state))\n",
    "    reward = rewardfun2(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[3],weibull_shape[3])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun3(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[3])*tp[3]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[3])*time_interval * math.ceil(tf[3]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step3(a_t,list(s_t))\n",
    "        r_t = rewardfun3(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step3(action , list(current_state))\n",
    "    reward = rewardfun3(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[4],weibull_shape[4])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun4(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[4])*tp[0]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[4])*time_interval * math.ceil(tf[4]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step4(a_t,list(s_t))\n",
    "        r_t = rewardfun4(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step4(action , list(current_state))\n",
    "    reward = rewardfun4(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[5],weibull_shape[5])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun5(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[5])*tp[5]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[5])*time_interval * math.ceil(tf[5]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step5(a_t,list(s_t))\n",
    "        r_t = rewardfun5(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step5(action , list(current_state))\n",
    "    reward = rewardfun5(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Steering Wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step6(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[6],weibull_shape[6])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun6(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[6])*tp[6]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[6])*time_interval * math.ceil(tf[6]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step6(a_t,list(s_t))\n",
    "        r_t = rewardfun6(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step6(action , list(current_state))\n",
    "    reward = rewardfun6(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Shifting Gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step7(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[7],weibull_shape[7])\n",
    "    if action == 0 :\n",
    "        st[0] +=5\n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "            \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun7(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[7])*tp[7]    \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[7])*time_interval * math.ceil(tf[7]/time_interval)\n",
    "    \n",
    "    if (st[1]== 0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q(s,a) and returns\n",
    "Q = {}\n",
    "returns ={}\n",
    "pi = defaultdict(lambda: 1/len(ALL_POSSIBLE_ACTIONS))# probability of action\n",
    "\n",
    "num_episodes=1000\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    Q[s] = {}\n",
    "    for a in ALL_POSSIBLE_ACTIONS:\n",
    "        Q[s][a] = 0\n",
    "        returns[(s,a)] = []\n",
    "\n",
    "for epi in range(num_episodes):# Looping through episodes \n",
    "    \n",
    "    # Initializes the state\n",
    "    s_t = reset1()   \n",
    "    epsilon = 1/(epi+1)\n",
    "    G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "            \n",
    "    for j in range(running_time//time_interval):\n",
    "        \n",
    "        a_t = choose_action(s_t , pi)\n",
    "        s_t = step7(a_t,list(s_t))\n",
    "        r_t = rewardfun7(a_t,s_t)\n",
    "            \n",
    "        seen_state_action_pairs = set()\n",
    "                 \n",
    "        state_action = (s_t,a_t)\n",
    "        G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "        if state_action not in seen_state_action_pairs: #first_visit\n",
    "                \n",
    "            returns[state_action].append(G) \n",
    "                \n",
    "            Q[s_t][a_t] = np.mean(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "            seen_state_action_pairs.add(state_action)\n",
    "                 \n",
    "            A_star, _ = max_dict(Q[s_t])# Finding the action with maximum value\n",
    "                \n",
    "                \n",
    "            for a in ALL_POSSIBLE_ACTIONS: # Update action probability for s_t in policy\n",
    "                if a == A_star:\n",
    "                    pi[(s_t,a)] = 1 - epsilon + (epsilon / len(ALL_POSSIBLE_ACTIONS))\n",
    "                else:\n",
    "                    pi[(s_t,a)] = (epsilon / len(ALL_POSSIBLE_ACTIONS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_replace = []\n",
    "current_state = reset1()\n",
    "for j in range(running_time//time_interval):\n",
    "    #choose a from pi\n",
    "    action = policy_using_pi(current_state, pi)\n",
    "    if action ==1:\n",
    "        time_replace.append(current_state)\n",
    "    obs = step7(action , list(current_state))\n",
    "    reward = rewardfun7(action,current_state)\n",
    "    current_state = obs\n",
    "\n",
    "np.unique(time_replace)[1]   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
